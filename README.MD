# Projeto LangFlow + Ollama + ChromaDB

## ðŸ“Œ DescriÃ§Ã£o
Este projeto tem como objetivo:

- Extrair dados de arquivos PDF

- Transformar e indexar os textos usando embeddings locais

- Permitir que uma IA local responda perguntas sobre os documentos

- Tudo Ã© executado localmente utilizando LangFlow, Ollama e ChromaDB.

## ðŸ›  Tecnologias utilizadas

LangFlow â€” Interface para criar fluxos de IA visualmente

Ollama â€” ExecuÃ§Ã£o de modelos LLM localmente

ChromaDB â€” Armazenamento vetorial para embeddings

PyMuPDF â€” Leitura e extraÃ§Ã£o de texto de PDFs

# ðŸ“‚ Estrutura de Arquivos
.
â”œâ”€â”€ requirements.txt     # DependÃªncias do projeto
â”œâ”€â”€ README.md            # DocumentaÃ§Ã£o
â””â”€â”€ fluxos/              # Arquivos exportados do LangFlow (.json)

# Como Rodar o Projeto

## 1. Criar ambiente virtual

python -m venv venv

source venv/bin/activate  # Mac/Linux

venv\Scripts\activate     # Windows

## 2. Instalar dependÃªncias

pip install -r requirements.txt

## 3. Instalar e iniciar o Ollama

siga as instruÃ§Ãµes: https://ollama.com/download

ollama serve

## 5. Baixar os modelos necessÃ¡rios

ollama pull gemma3:4b

ollama pull nomic-embed-text

## 5. Rodar o LangFlow

langflow run

O Localhost vai aparecer no terminal ou LangFlow abrirÃ¡ no navegador.

Carregue o fluxo (.json) clicando em "upload a flow " e execute o projeto.

## 6. Dentro do LangFlow

Inserir os PDFs desejados no bloco "File", executar o ChromaDB

Iniciar o chat clicando em "Playground"

